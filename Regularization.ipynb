{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVH2v9VCnT0AbgXq/7l9ET",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farah-Deeba-UNCC/Introduction-to-ML/blob/main/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSicIVMArulh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L2 Regularization\n",
        "L2 regularization, also known as Ridge Regression, is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. This penalty term shrinks the magnitude of the model coefficients, making the model more generalizable to unseen data.\n",
        "\n",
        "Mathematical Formulation\n",
        "For a linear regression model, the objective function with L2 regularization is given by:\n",
        "$J(W) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} (y^{(i)}- h_\\theta(x^{(i)}))^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right]$\n",
        "\n",
        "The first term is the mean of squared errors (MSE), and the second term is the L2 penalty.\n",
        "Ridge regression discourages large coefficient values and reducing model complexity and improves generalization. Unlike L1 regularization (Lasso), which can force some weights to be exactly zero, L2 regularization shrinks all coefficients but does not make them zero. L2 regularization tends to distribute weight values more evenly across features, making it useful when dealing with high-dimensional data.\n",
        "\n",
        "By tuning the ùúÜ parameter, one can control the extent of regularization. A higher ùúÜ leads to more penalty and a simpler model, while a lower Œª results in a model that fits the training data more closely."
      ],
      "metadata": {
        "id": "3WBiZiILuHdJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA0_fPjaoklq"
      },
      "outputs": [],
      "source": [
        "# L2- regularization (ridge)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from ipywidgets import interact\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "x = np.linspace(-3, 3, 30)\n",
        "y = 2 * x**3 + x**2 - 2*x + np.random.normal(0, 5, size=x.shape)  # More noisy cubic function\n",
        "x = x.reshape(-1, 1)\n",
        "\n",
        "# Function to visualize the effect of L2 regularization\n",
        "def plot_ridge(alpha):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Fit Linear Regression with high-degree polynomial (degree=10)\n",
        "    model_lr = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())\n",
        "    model_lr.fit(x, y)\n",
        "    y_pred_lr = model_lr.predict(np.linspace(-3, 3, 100).reshape(-1, 1))\n",
        "\n",
        "    # Fit Ridge Regression with L2 regularization\n",
        "    model_ridge = make_pipeline(PolynomialFeatures(degree=10), Ridge(alpha=alpha))\n",
        "    model_ridge.fit(x, y)\n",
        "    y_pred_ridge = model_ridge.predict(np.linspace(-3, 3, 100).reshape(-1, 1))\n",
        "\n",
        "    # Plot regression results\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(x, y, label=\"Data\", color='red')\n",
        "    plt.plot(np.linspace(-3, 3, 100), y_pred_lr, label=\"Linear Regression (no regularization)\", linestyle='dashed', linewidth=2)\n",
        "    plt.plot(np.linspace(-3, 3, 100), y_pred_ridge, label=f\"Ridge Regression (alpha={alpha})\", linewidth=2)\n",
        "\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.title(\"Effect of L2 Regularization on Data Fitting\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Show effect on model coefficients\n",
        "    plt.subplot(1, 2, 2)\n",
        "    coefs_lr = model_lr.named_steps['linearregression'].coef_.flatten()\n",
        "    coefs_ridge = model_ridge.named_steps['ridge'].coef_.flatten()\n",
        "    plt.plot(range(len(coefs_lr)), coefs_lr, 'bo-', label=\"Linear Regression Coefficients\")\n",
        "    plt.plot(range(len(coefs_ridge)), coefs_ridge, 'ro-', label=f\"Ridge Coefficients (alpha={alpha})\")\n",
        "\n",
        "    plt.xlabel(\"Coefficient Index\")\n",
        "    plt.ylabel(\"Coefficient Value\")\n",
        "    plt.title(\"Effect of Regularization on Coefficients\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        " # Print theta values\n",
        "    print(f\"Theta values for Ridge Regression (alpha={alpha}):\")\n",
        "    print(coefs_ridge)\n",
        "# Interactive widget to adjust alpha (regularization strength)\n",
        "interact(plot_ridge, alpha=(0.001, 100.0, 1.0));\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### L1 regularization,\n",
        "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), is a technique used in machine learning to prevent overfitting by adding a penalty equal to the absolute value of the magnitude of coefficients to the loss function. This encourages sparsity in the model parameters, effectively reducing the number of features used in the model.\n",
        "\n",
        "Mathematical Formulation:\n",
        "In a linear regression model, the L1 regularization modifies the cost function as follows:\n",
        "$J(W) = \\frac{1}{2m} \\left[ \\sum_{i=1}^{m} (y^{(i)}- h_\\theta(x^{(i)}))^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j| \\right]$\n",
        "\n",
        "\n",
        "L1 regularization can shrink some coefficients to exactly zero, effectively performing feature selection by eliminating less informative features. The introduction of the absolute value penalty promotes sparsity in the model. It is useful in scenarios where feature selection is desired. By penalizing large coefficients, L1 regularization reduces model complexity abd helps to prevent overfitting.\n",
        "\n",
        "The regularization parameter Œª balances the trade-off between fitting the training data well and keeping the number of model coefficients small to maintain generalization."
      ],
      "metadata": {
        "id": "oCqU3zxgv3Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L1-regularization (Lasso)\n",
        "from sklearn.linear_model import Lasso, LinearRegression\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "x = np.linspace(-3, 3, 30)\n",
        "y = 2 * x**3 + x**2 - 2*x + np.random.normal(0, 5, size=x.shape)  # More noisy cubic function\n",
        "x = x.reshape(-1, 1)\n",
        "\n",
        "# Function to visualize the effect of L1 regularization\n",
        "def plot_lasso(alpha):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Fit Linear Regression with high-degree polynomial (degree=10)\n",
        "    model_lr = make_pipeline(PolynomialFeatures(degree=10), LinearRegression())\n",
        "    model_lr.fit(x, y)\n",
        "    y_pred_lr = model_lr.predict(np.linspace(-3, 3, 100).reshape(-1, 1))\n",
        "\n",
        "    # Fit Lasso Regression with L1 regularization\n",
        "    model_lasso = make_pipeline(PolynomialFeatures(degree=10), Lasso(alpha=alpha, max_iter=5000))\n",
        "    model_lasso.fit(x, y)\n",
        "    y_pred_lasso = model_lasso.predict(np.linspace(-3, 3, 100).reshape(-1, 1))\n",
        "\n",
        "    # Plot regression results\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(x, y, label=\"Data\", color='red')\n",
        "    plt.plot(np.linspace(-3, 3, 100), y_pred_lr, label=\"Linear Regression (no regularization)\", linestyle='dashed', linewidth=2)\n",
        "    plt.plot(np.linspace(-3, 3, 100), y_pred_lasso, label=f\"Lasso Regression (alpha={alpha})\", linewidth=2)\n",
        "\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.title(\"Effect of L1 Regularization on Data Fitting\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Show effect on model coefficients\n",
        "    plt.subplot(1, 2, 2)\n",
        "    coefs_lr = model_lr.named_steps['linearregression'].coef_.flatten()\n",
        "    coefs_lasso = model_lasso.named_steps['lasso'].coef_.flatten()\n",
        "    plt.plot(range(len(coefs_lr)), coefs_lr, 'bo-', label=\"Linear Regression Coefficients\")\n",
        "    plt.plot(range(len(coefs_lasso)), coefs_lasso, 'ro-', label=f\"Lasso Coefficients (alpha={alpha})\")\n",
        "\n",
        "    plt.xlabel(\"Coefficient Index\")\n",
        "    plt.ylabel(\"Coefficient Value\")\n",
        "    plt.title(\"Effect of Regularization on Coefficients\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        " # Print theta values\n",
        "    print(f\"Theta values for Lasso Regression (alpha={alpha}):\")\n",
        "    print(coefs_lasso)\n",
        "# Interactive widget to adjust alpha (regularization strength)\n",
        "interact(plot_lasso, alpha=(0.001, 100.0, 1.0));\n"
      ],
      "metadata": {
        "id": "gbXbj0Z_rv5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
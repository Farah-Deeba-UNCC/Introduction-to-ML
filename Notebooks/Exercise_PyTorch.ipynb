{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farah-Deeba-UNCC/Introduction-to-ML/blob/main/Notebooks/Exercise_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HHimAnKwkt9a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# t_c is the temperature in celcius; this is our known output.\n",
        "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
        "# t_u is the temperature in unknown unit; this is our feature.\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "\n",
        "# Try yourself: Convert these arrays into tensors.\n",
        "# t_c =\n",
        "# t_u ="
      ],
      "metadata": {
        "id": "8eTb6ZEAlCvM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model definition based on weight w and bias b\n",
        "def model(t_u, w, b):\n",
        "    return w * t_u + b"
      ],
      "metadata": {
        "id": "V9G3H8urmnzO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss definition\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()"
      ],
      "metadata": {
        "id": "YOwBR2iDmxwz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.ones(())\n",
        "b = torch.zeros(())\n",
        "\n",
        "# t_p is the predicted output for the current value of w and b\n",
        "t_p = model(t_u, w, b)\n",
        "# Try yourself: print the predicted values t_p"
      ],
      "metadata": {
        "id": "6vKPafzGm7q8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(t_p, t_c)\n",
        "# Try yourself: print loss"
      ],
      "metadata": {
        "id": "rKIJEkO5nA8w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's normalize the input; you can use the standard way to do normalization; Here we have done a crude normalization so that our two featues are in the similar range.\n",
        "t_un = 0.1*t_u"
      ],
      "metadata": {
        "id": "rkFv0tNvmN9e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we define an optimizer for optimizing the parameters of our linear model\n",
        "import torch.optim as optim\n",
        "params = torch.tensor([w,b],requires_grad=True)  # notice that requires_grad defines that it is associated with a graph and gradeint will be computed when we call the step function.\n",
        "learning_rate = 1e-5\n",
        "optimizer = optim.SGD([params],lr = learning_rate)\n",
        "# try yourself: print the params.\n",
        "# try yourself: print the gradient."
      ],
      "metadata": {
        "id": "JGcsHMlQoxzS"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do one iteration of training: one forward and one backward pass to update the gradient and the parameters.\n",
        "t_p = model(t_u,*params)\n",
        "loss = loss_fn(t_p,t_c)\n",
        "loss.backward()\n",
        "# try yourself: print the params. Have the values been updated?\n",
        "# try yourself: print the gradient. Have the values been updated?\n",
        "optimizer.step()\n",
        "# try yourself: print the params. Have the values been updated?\n",
        "# try yourself: print the gradient. Have the values been updated?\n",
        "\n"
      ],
      "metadata": {
        "id": "nzwpyo_XqX6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will do training for 5000 iteration to find the optimum value of the parameters.\n",
        "def training_loop(n_epochs, optimizer, params, t_u,t_c):\n",
        "  for epoch in range(1,n_epochs+1):\n",
        "    t_p = model(t_u, *params)\n",
        "    loss = loss_fn(t_p,t_c)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "      print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
        "      print(params)\n",
        "      print(params.grad)\n",
        "  return params"
      ],
      "metadata": {
        "id": "vZ2NQai2sF4b"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([w,b],requires_grad=True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params],lr = learning_rate)\n",
        "# Try yourself: call training_loop function for 5000 epochs.\n",
        "\n",
        "# Try yourself: print the params values"
      ],
      "metadata": {
        "id": "q91PZCu_sbWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Validation in PyTorch"
      ],
      "metadata": {
        "id": "Tw2UlHsX8jPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting a dataset into 8:2 ratio\n",
        "n_samples = t_u.shape[0]\n",
        "n_val = int(0.2 * n_samples)\n",
        "\n",
        "shuffled_indices = torch.randperm(n_samples)\n",
        "\n",
        "train_indices = shuffled_indices[:-n_val]\n",
        "val_indices = shuffled_indices[-n_val:]\n",
        "# Try yourself: print train_indices and val_indices. see if the number of training and validation match with our intended ratio"
      ],
      "metadata": {
        "id": "532hOXol8sZ4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_u = t_u[train_indices]\n",
        "train_t_c = t_c[train_indices]\n",
        "\n",
        "val_t_u = t_u[val_indices]\n",
        "val_t_c = t_c[val_indices]"
      ],
      "metadata": {
        "id": "A5KBfsaS_Odd"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crude normalization\n",
        "train_t_un = 0.1 * train_t_u\n",
        "val_t_un = 0.1 * val_t_u"
      ],
      "metadata": {
        "id": "HMT_IHeF_hr5"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(n_epochs, optimizer, params, train_t_u,val_t_u, train_t_c,val_t_c):\n",
        "  for epoch in range(1,n_epochs+1):\n",
        "\n",
        "    # Try yourself: identif (i) where we are doing forward pass on the training data,\n",
        "    # (ii)  where we are doing forward pass on the validation data,\n",
        "    # (iii) where we are computing the gradient on training data\n",
        "    # (iv) and where we are updating the paramter values on training data (iii and iV constitute the backward pass)\n",
        "    # (v) where we are doing the backwass pass on validation data\n",
        "\n",
        "    train_t_p = model(train_t_u, *params)\n",
        "    train_loss = loss_fn(train_t_p, train_t_c)\n",
        "\n",
        "    val_t_p = model(val_t_u, *params)\n",
        "    val_loss = loss_fn(val_t_p, val_t_c)\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch<= 3 or epoch % 500 == 0:\n",
        "      print('Epoch %d, Training Loss %f, Validation Loss %f' % (epoch, float(train_loss), float(val_loss)))\n",
        "      print(params)\n",
        "      print(params.grad)\n",
        "  return params"
      ],
      "metadata": {
        "id": "t5xaFzi_AEAw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = torch.tensor([1.0,0.0],requires_grad = True)\n",
        "learning_rate = 1e-2\n",
        "optimizer = optim.SGD([params],lr = learning_rate)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    params = params,\n",
        "    train_t_u = train_t_un,\n",
        "    val_t_u = val_t_un,\n",
        "    train_t_c = train_t_c,\n",
        "    val_t_c = val_t_c)\n",
        "\n",
        "# try yourself: print the final values of the parameters. What will be the predicted output if the temperature at unknow unit is 80."
      ],
      "metadata": {
        "id": "_oIEW7gei7k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "t_p = model(t_un, *params)\n",
        "fig = plt.figure(dpi=600)\n",
        "plt.xlabel(\"Temperature (Â°Fahrenheit)\")\n",
        "plt.ylabel(\"Temperature (Â°Celsius)\")\n",
        "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
        "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
      ],
      "metadata": {
        "id": "8KukuwAKnSUP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Farah-Deeba-UNCC/Introduction-to-ML/blob/main/Notebooks/9-LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKZJhD4KbjKq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem statement\n",
        "Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand next.\n",
        "\n",
        "## Load data\n",
        " The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss. The dataset is loaded from the data file into the variables X and Y:"
      ],
      "metadata": {
        "id": "YryJkRp_-1IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df =pd.read_csv('https://raw.githubusercontent.com/satishgunjal/datasets/master/univariate_profits_and_populations_from_the_cities.csv')\n",
        "df.head() # To get first n rows from the dataset default value of n is 5"
      ],
      "metadata": {
        "id": "kFeqxGMobnHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot data\n",
        "Before starting on any task, it is often useful to understand the data by visualizing it. For this dataset, we can use a scatter plot to visualize the data, since it has only two variables (profit and population) to plot (a simple linear regression problem) .\n",
        "\n",
        "Note that many problems in real life are multi-dimensional (multiple linear regression problem) and can’t be plotted on a 2-d plot.\n",
        "\n",
        "Now let's create a scatter plot of the data:"
      ],
      "metadata": {
        "id": "5Q-9OJyRAKXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df.population,df.profit)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XK3uyKb7AETe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df.population,df.profit, color='red',marker= '+')\n",
        "plt.grid()\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.title('Scatter plot of training data')"
      ],
      "metadata": {
        "id": "OzzrqPKaBORa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizatin function\n",
        " The objective of the linear regression is to minimize the cost function:\n",
        "\n",
        " $J(\\theta)= \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x_i)-y_i)^2$\n",
        "\n",
        " where\n",
        " $h_\\theta(x_i) = \\theta_0+\\theta_1 x$\n",
        "\n",
        " Threefore, according to the gradient descent algorithm\n",
        "\n",
        " $\\theta_j := \\theta_j -\\alpha \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x_i)-y_i)x_{ij}$\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "zhVkggmQsyx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization\n",
        " To take into account the intercept term , we add an additional first column to df and set it to all ones.\n",
        "\n",
        " In the following lines, we add another dimension to our data to accommodate the  intercept term. We also initialize the initial parameters to 0 and the learning rate alpha to 0.01"
      ],
      "metadata": {
        "id": "ZBNRNCk5FYB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets create a matrix with a dimension of m by 1. m is the number of observations\n",
        "m = df.population.size\n",
        "X_0 = np.ones((m, 1))\n",
        "X_0 = pd.DataFrame(X_0)\n",
        "df.insert(0, \"intercept\",X_0)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "Ffq4iq7TcB8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.loc[:,['intercept','population']]\n",
        "y = df.profit\n",
        "theta = np.zeros(2)\n",
        "iterations = 1500;\n",
        "alpha = 0.01;"
      ],
      "metadata": {
        "id": "VRuJ27IscLZ2"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function, computeCost, that will compute the cost for a given set of $\\theta$ values;"
      ],
      "metadata": {
        "id": "2rCtthWRHStb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeCost(X,y,theta):\n",
        "  \"\"\"\n",
        "  Compute cost for linear regression.\n",
        "  Input Parameters\n",
        "  ----------------\n",
        "  X : 2D array where each row represent the training example and each column represent\n",
        "  m= number of training examples\n",
        "  n= number of features (including X_0 column of ones)\n",
        "  y : 1D array of labels/target value for each traing example. dimension(1 x m)\n",
        "  theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
        "  Output Parameters\n",
        "  -----------------\n",
        "  J : Scalar value.\n",
        "  \"\"\"\n",
        "  m = len(X)\n",
        "  predictions = X.dot(theta)\n",
        "  errors = np.subtract(predictions, y)\n",
        "  sqrErrors = np.square(errors)\n",
        "  J = 1 / (2 * m) * np.sum(sqrErrors)\n",
        "  return J\n"
      ],
      "metadata": {
        "id": "guvEkG-oHf19"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets compute the cost for theta values\n",
        "print('The cost for given values of theta_0 and theta_1 =',cost )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfgq5KYscVUY",
        "outputId": "dad9892c-bab1-4a01-a368-665e25cfb178"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cost for given values of theta_0 and theta_1 = 32.072733877455676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function, gradientDescent to implement the gradient descent algorithm and optimize the $\\theta$ values."
      ],
      "metadata": {
        "id": "dBeZADDeLrF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(X, y, theta, alpha, iterations):\n",
        "  \"\"\"\n",
        "  Compute cost for linear regression.\n",
        "\n",
        "  Input Parameters\n",
        "  ----------------\n",
        "  X : 2D array where each row represent the training example and each column represent\n",
        "      m= number of training examples\n",
        "      n= number of features (including X_0 column of ones)\n",
        "  y : 1D array of labels/target value for each traing example. dimension(m x 1)\n",
        "  theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n",
        "  alpha : Learning rate. Scalar value\n",
        "  iterations: No of iterations. Scalar value.\n",
        "\n",
        "  Output Parameters\n",
        "  -----------------\n",
        "  theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n)\n",
        "  cost_history: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)  \"\"\"\n",
        "  cost_history = np.zeros(iterations)\n",
        "\n",
        "  for i in range(iterations):\n",
        "    predictions = X.dot(theta)\n",
        "    errors = np.subtract(predictions, y)\n",
        "    sum_delta = (alpha / m) * X.transpose().dot(errors);\n",
        "    theta = theta - sum_delta;\n",
        "    cost_history[i] = computeCost(X, y, theta)\n",
        "  return theta, cost_history"
      ],
      "metadata": {
        "id": "X_qMp20VcZi5"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent\n",
        "We optimize the cost function parameters by calling gradientDescent function. A good way to verify that gradient descent is working correctly is to look at the value of  and check that it is decreasing with each step. We print the cost after every hundred iteration to keep things short.\n",
        "\n",
        "The cost $J(\\theta)$  is parameterized by the vector θ, not X and y. That is, we minimize the value of $J(\\theta)$  by changing the values of the vector θ, not by changing X or y. Value of $J(\\theta)$  should never increase, and should converge to a steady value by the end of the algorithm."
      ],
      "metadata": {
        "id": "sVF6nPkwL7MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta, cost_history = gradientDescent(X, y, theta, alpha, iterations)\n",
        "print('Final value of theta =', theta)\n",
        "print('cost_history =', cost_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAJEHsy2clT2",
        "outputId": "b6b9bd90-46fa-4d80-9aac-b53141cf97ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final value of theta = intercept    -3.630291\n",
            "population    1.166362\n",
            "dtype: float64\n",
            "cost_history = [6.73719046 5.93159357 5.90115471 ... 4.48343473 4.48341145 4.48338826]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since X is list of list (feature matrix) lets take values of column of index 1 only\n",
        "plt.scatter(X.population, y, color='red', marker= '+', label= 'Training Data')\n",
        "plt.plot(X.population, X.dot(theta), color='green', label='Linear Regression')\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "plt.grid()\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.title('Linear Regression Fit')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "oB3-JFqSclaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, iterations + 1),cost_history, color='blue')\n",
        "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
        "plt.grid()\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.ylabel('Cost (J)')\n",
        "plt.title('Convergence of gradient descent')"
      ],
      "metadata": {
        "id": "ylFRSf66cr2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}